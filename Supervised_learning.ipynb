{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to get experience with supervised classification algorithms <br>\n",
    "and multilayer feed forward neural networks. <br>\n",
    "The datasets will be synthetic and I will implement the methods without the use of ML libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Comparing classifiers\n",
    "## Datasets\n",
    "We start by making a synthetic dataset of 1600 datapoints and three classes, with 800 individuals in one class and 400 in each of the two other classes.\n",
    "Since this data set is synthetic it is not necessary to shuffle it since that is already done by scikit, but it would be useful to shuffle it if it was real world data with no other obvious way to split it.\n",
    "\n",
    "We should split the data so that we keep the alignment between X and t, which may be achieved by shuffling the indices. We split into 50% for training, 25% for validation, and 25% for final testing. The set for final testing *will not be used* till the end of the project.\n",
    "\n",
    "We fix the seed both for data set generation and for shuffling, so that we work on the same datasets when we rerun the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, t = make_blobs(n_samples=[400,800,400], centers=[[0,0],[1,2],[2,3]], \n",
    "                  n_features=2, random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1301,  293,  968,  624,  658,  574,  433,  368,  512,  353])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.arange(X.shape[0])\n",
    "random.seed(2020)\n",
    "random.shuffle(indices)\n",
    "indices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[indices[:800],:]\n",
    "X_val = X[indices[800:1200],:]\n",
    "X_test = X[indices[1200:],:]\n",
    "t_train = t[indices[:800]]\n",
    "t_val = t[indices[800:1200]]\n",
    "t_test = t[indices[1200:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will  make a second dataset by merging the two smaller classes in (X,t) and call the new set (X, t2). This will be a binary set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_train = t_train == 1\n",
    "t2_train = t2_train.astype('int')\n",
    "t2_val = (t_val == 1).astype('int')\n",
    "t2_test = (t_test == 1).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the two training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f105a1450d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your solution\n",
    "plt.scatter(*zip(*X_train), c=\"indigo\")\n",
    "plt.scatter(*zip(*X_train[t2_train.astype(\"bool\")]), c=\"violet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "We see that that set (X, t2) is far from linearly separable, and we will explore how various classifiers are able to handle this. We start with linear regression.\n",
    "\n",
    "We will train the classifier on (X_train, t2_train) and test for accuracy on (X_val, t2_val) for various values of *diff*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6075\n",
      "0.6075\n",
      "0.6075\n",
      "0.445\n"
     ]
    }
   ],
   "source": [
    "class NumpyClassifier():\n",
    "    \"\"\"Common methods to all numpy classifiers --- if any\"\"\"\n",
    "    \n",
    "    def accuracy(self, X_testing, y_test, **kwargs):\n",
    "        pred = self.predict(X_testing, **kwargs)\n",
    "        if len(pred.shape) > 1:\n",
    "            pred = pred[:,0]\n",
    "        return sum(pred==y_test)/len(pred)\n",
    "    \n",
    "def add_bias(X):\n",
    "    \"\"\"X is a Nxm matrix: N datapoints, m features\n",
    "    Return a Nx(m+1) matrix with added bias in position zero\"\"\"\n",
    "    sh = X.shape\n",
    "    m = sh[0]\n",
    "    bias = np.ones((m,1)) # Make a m*1 matrix of 1-s\n",
    "    # Concatenate the column of bias in front of the columns of X.\n",
    "    return np.concatenate((bias, X), axis = 1) \n",
    "\n",
    "class NumpyLinRegClass(NumpyClassifier):\n",
    "\n",
    "    def fit(self, X_train, t_train, eta = 0.1, epochs=10, diff=0.001):\n",
    "        \"\"\"X_train is a Nxm matrix, N data points, m features\n",
    "        t_train are the targets values for training data\"\"\"\n",
    "        \n",
    "        (k, m) = X_train.shape\n",
    "        X_train = add_bias(X_train)\n",
    "        \n",
    "        self.weights = weights = np.zeros(m+1)\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            change = eta / k *  X_train.T @ (X_train @ weights - t_train)     \n",
    "            weights -= change\n",
    "            \n",
    "            if sum(abs(weights)) < diff:\n",
    "                break\n",
    "    \n",
    "    def predict(self, x, threshold=0.5):\n",
    "        z = add_bias(x)\n",
    "        score = z @ self.weights\n",
    "        return score>threshold    \n",
    "\n",
    "thresholds = [0.000000000001, 0.1, 0.2, 0.3]\n",
    "\n",
    "for x in thresholds:\n",
    "    cls = NumpyLinRegClass()\n",
    "    cls.fit(X_train, t2_train, epochs=10000, diff=x)\n",
    "    print(cls.accuracy(X_val, t2_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.2 seems like a reasonable value for diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "Now we do the same thing with logistic regression, i.e., add the *diff*, tune it, report accuracy, and store it for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59\n",
      "0.59\n",
      "0.59\n",
      "0.59\n",
      "0.59\n"
     ]
    }
   ],
   "source": [
    "class NumpyClassifier():\n",
    "    \"\"\"Common methods to all numpy classifiers --- if any\"\"\"\n",
    "    \n",
    "    def accuracy(self,X_test, y_test, **kwargs):\n",
    "        pred = self.predict(X_test, **kwargs)\n",
    "        if len(pred.shape) > 1:\n",
    "            pred = pred[:,0]\n",
    "        return sum(pred==y_test)/len(pred)\n",
    "\n",
    "def logistic(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "class NumpyMultiLogReg(NumpyClassifier):\n",
    "\n",
    "    def fit(self, X_train, t_train, eta = 0.1, epochs=10, diff=0.001):\n",
    "        \"\"\"X_train is a Nxm matrix, N data points, m features\n",
    "        t_train are the targets values for training data\n",
    "        \n",
    "        Makes a fit for each feature, t_train now has multiple values\"\"\"\n",
    "        \n",
    "        (k, m) = X_train.shape\n",
    "        X_train = add_bias(X_train)\n",
    "        \n",
    "        self.weights = weights = np.zeros(m+1)\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            change = eta / k *  X_train.T @ (self.forward(X_train) - t_train)\n",
    "            weights -= change\n",
    "            \n",
    "            if sum(abs(change)) < diff:\n",
    "                break\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return logistic(X @ self.weights)\n",
    "    \n",
    "    def score(self, x):\n",
    "        z = add_bias(x)\n",
    "        score = self.forward(z)\n",
    "        return score\n",
    "    \n",
    "    def predict(self, x, threshold=0.5):\n",
    "        z = add_bias(x)\n",
    "        score = self.forward(z)\n",
    "        return (score>threshold).astype('int')\n",
    "    \n",
    "thresholds = [1e-09, 0.001, 0.004, 0.01, 0.1]\n",
    "\n",
    "for x in thresholds:\n",
    "    cls = NumpyMultiLogReg()\n",
    "    cls.fit(X_train, t_train, epochs=1000, diff=x)\n",
    "    print(cls.accuracy(X_val, t_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *k*-nearest neighbors (*k*NN)\n",
    "We will now compare to the *k*-nearest neighbors classifier.\n",
    "\n",
    "We train on (X_train, t2_train) and test on (X2_val, t2_val) for various values of *k*. Choose the best *k*, report accuracy and store for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68\n",
      "0.7525\n",
      "0.7625\n",
      "0.7575\n",
      "0.7525\n",
      "0.75\n",
      "0.745\n"
     ]
    }
   ],
   "source": [
    "def distance_L2(a, b):\n",
    "    \"L2-distance using comprehension\"\n",
    "    s = sum((x - y) ** 2 for (x,y) in zip(a,b))\n",
    "    return s ** 0.5\n",
    "\n",
    "def majority(a):\n",
    "    counts = Counter(a)\n",
    "    return counts.most_common()[0][0]\n",
    "\n",
    "class PyClassifier():\n",
    "    \"\"\"Common methods to all python classifiers --- if any\"\"\"\n",
    "    \n",
    "    def accuracy(self, X_test, y_test, **kwargs):\n",
    "        \"\"\"Calculate the accuracy of the classifier \n",
    "        using the predict method\"\"\"\n",
    "        predicted = [self.predict(a, **kwargs) for a in X_test]\n",
    "        equal = len([(p, g) for (p,g) in zip(predicted, y_test) if p==g])\n",
    "        return equal / len(y_test)\n",
    "\n",
    "class PykNNClassifier(PyClassifier):\n",
    "    \"\"\"kNN classifier using pure python representations\"\"\"\n",
    "    \n",
    "    def __init__(self, k=3, dist=distance_L2):\n",
    "        self.k = k\n",
    "        self.dist = dist\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    \n",
    "    def predict(self, a):\n",
    "        X = self.X_train\n",
    "        y = self.y_train\n",
    "        distances = [(self.dist(a, b), b, c) for (b, c) in zip(X, y)]\n",
    "        distances.sort()\n",
    "        predictors = [c for (_,_,c) in distances[0: self.k]]\n",
    "        return majority(predictors)\n",
    "\n",
    "# testing\n",
    "for k in [3,19,20,21,25, 30, 100]:\n",
    "    cls = PykNNClassifier(k)\n",
    "    cls.fit(list(X_train), list(t2_train))\n",
    "    print(cls.accuracy(list(X_val), list(t2_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k=20 seems to do the best job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple perceptron\n",
    "Finally, we run a simple perceptron on the same set, and report and store accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6475\n"
     ]
    }
   ],
   "source": [
    "class PyPerClassifier(PyClassifier):\n",
    "    \"\"\"Simple perceptron python classifier\"\"\"\n",
    "    \n",
    "    def fit(self, X_train, y_train, eta=1, epochs=1):\n",
    "        \"\"\"Train the self.weights on the training data with learning\n",
    "        rate eta, running epochs many epochs\"\"\"\n",
    "        X_train = [[1]+list(x) for x in X_train] # Put bias in position 0      \n",
    "        self.dim = dim = len(X_train[0])\n",
    "        self.weights = weights = [0 for _ in range(dim)]\n",
    "        # Initialize all weights to 0.\n",
    "\n",
    "        for e in range(epochs):\n",
    "            for x, t in zip(X_train, y_train):\n",
    "                y = int(self.forward(x)>0)\n",
    "                for i in range(dim):\n",
    "                    weights[i] -= eta * (y - t) * x[i]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Calculate the score for the item x\"\"\"\n",
    "        score = sum([self.weights[i]*x[i] for i in range(self.dim)])\n",
    "        return score       \n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict the value for the item x\"\"\"\n",
    "        x = [1] + list(x)\n",
    "        score = self.forward(x)\n",
    "        return int(score > 0)\n",
    "\n",
    "# testing\n",
    "cls = PyPerClassifier()\n",
    "cls.fit(list(X_train), list(t2_train))\n",
    "print(cls.accuracy(list(X_val), list(t2_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Here are the accuracies for the four classifiers in a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0lax\">Name</th>\n",
    "    <th class=\"tg-0lax\">Accuracy</th>\n",
    "    <th class=\"tg-0lax\">Parameters</th>\n",
    "    <th class=\"tg-0lax\">Comments</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Linear regression</td>\n",
    "    <td class=\"tg-0pky\">0.6075</td>\n",
    "    <td class=\"tg-0pky\">diff = 0.2</td>\n",
    "    <td class=\"tg-0lax\">No difference as long as diff &lt; 0.2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Logisitic regression</td>\n",
    "    <td class=\"tg-0pky\">0.64</td>\n",
    "    <td class=\"tg-0pky\">diff = 0.004</td>\n",
    "    <td class=\"tg-0lax\">This was worse for both higher and lower values of diff</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">kNN</td>\n",
    "    <td class=\"tg-0pky\">0.7625</td>\n",
    "    <td class=\"tg-0pky\">k = 20</td>\n",
    "    <td class=\"tg-0lax\">This was also worse for both more and fewer neighbours</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Perceptron</td>\n",
    "    <td class=\"tg-0pky\">0.745</td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0lax\">This was fairly quick, as expected, but I hoped it would beat kNN</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classifiers\n",
    "We turn to the task of classifying when there are more than two classes, and the task is to ascribe one class to each input. We will now use the set (X, t)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *k*NN\n",
    "The *k*-nearest neighbors classifier can also handle multiple classes without needing to be modified. We will train it on (X_train, t_train), test it on (X_val, t_val) for various values of *k*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.675\n",
      "0.755\n",
      "0.7675\n",
      "0.76\n",
      "0.7525\n",
      "0.755\n",
      "0.7475\n"
     ]
    }
   ],
   "source": [
    "for k in [3,19,20,21,25, 30, 100]:\n",
    "    cls = PykNNClassifier(k)\n",
    "    cls.fit(list(X_train), list(t_train))\n",
    "    print(cls.accuracy(list(X_val), list(t_val)))\n",
    "    \n",
    "# again it seems like k=20 is the better option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression \"one-vs-rest\"\n",
    "A logistic regression classifier can be turned into a multi-class classifier using the one-vs-rest approach. We train one classifier for each class and assign the class which ascribes the highest probability.\n",
    "\n",
    "We will also extend the logisitc regression classifier to a multi-class classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.285"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for every feature\n",
    "# define one-vs-rest\n",
    "# fit training data and predict\n",
    "\n",
    "for num in range(3):\n",
    "    t3_train = (t_train == num).astype(\"int\")\n",
    "\n",
    "class NumpyMultiLogReg(NumpyClassifier):\n",
    "\n",
    "    def fit(self, X_train, t_train, eta = 0.1, epochs=10, diff=0.001):\n",
    "        \n",
    "        # Fits a training set for each feature\n",
    "        self.features = set(t_train)\n",
    "        t_list_features = []\n",
    "        (k, m) = X_train.shape\n",
    "        self.weights = np.zeros((len(self.features), m+1))\n",
    "        X_train = add_bias(X_train)\n",
    "        \n",
    "        for f in self.features:\n",
    "            t_feature = (t_train == f).astype(\"int\")\n",
    "            t_list_features.append(t_feature)\n",
    "            \n",
    "            self.weights[f] = weights = np.zeros(m+1)\n",
    "            \n",
    "            for e in range(epochs):\n",
    "                change = eta / k *  X_train.T @ (self.forward(X_train, f) - t_list_features[f])\n",
    "                weights -= change\n",
    "            \n",
    "                if sum(abs(change)) < diff:\n",
    "                    break\n",
    "    \n",
    "    def forward(self, X, f):\n",
    "        return logistic(X @ self.weights[f])\n",
    "    \n",
    "    def score(self, x):\n",
    "        z = add_bias(x)\n",
    "        score = self.forward(z)\n",
    "        return score\n",
    "    \n",
    "    def predict(self, x, threshold=0.5):\n",
    "        z = add_bias(x)\n",
    "        score = []\n",
    "        predicted = np.zeros(len(x))\n",
    "        # check the probability for each feature\n",
    "        for f in self.features:\n",
    "            score.append(self.forward(z,f))\n",
    "        # decide which feature the point is the closest to\n",
    "        for i in range(len(x)):\n",
    "            best = 0\n",
    "            # find the highest probability\n",
    "            for f in self.features:\n",
    "                if score[f][i] > best:\n",
    "                    best = score[f][i]\n",
    "                    # assign that feature as the predicted for the chosen point\n",
    "                    predicted[i] = f\n",
    "        return predicted\n",
    "\n",
    "cls = NumpyMultiLogReg()\n",
    "cls.fit(X_train, t_train)\n",
    "cls.accuracy(X_val, t_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations so far:\n",
    "* The kNN does better than the one-vs-rest approach to logistic classification above\n",
    "* The three-class classifier does worse than the binary one, I expected it to score above 1/3\n",
    "* I would assume one of the reasons is that the three-class classifier will be penalized more as there are more possibilities for false classifications, e.g. if a point which was feature 2 was classifed as feature 0 the three-class classifier is wrong to classify it as such, whereas the binary classifier can be right by just classifying it as \"not feature 0\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding non-linear features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are returning to the binary classifier and the set (X, t2). <br>\n",
    "As we see, some of the classifiers are not doing too well on the (X, t2) set. <br>\n",
    "It is easy to see from the plot that this data set is not well suited for linear classifiers.<br>\n",
    "There are several possible options for trying to learn on such a set. One is to construct new features from the original features to get better discriminants. <br>\n",
    "This works e.g., for the XOR-problem. The current classifiers use two features: $x_1$ and $x_2$ (and a bias term $x_0$). We will try to add three additional features of the form ${x_1}^2$, ${x_2}^2$, $x_1*x_2$ to the original features and see what the accuracies are now.\n",
    "(Some of the classifiers could probably achieve better results if we scaled the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with non-linear features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5275\n",
      "0.5275\n",
      "0.5275\n",
      "0.5275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45479/736322123.py:37: RuntimeWarning: overflow encountered in matmul\n",
      "  change = eta / k *  X_train.T @ (X_train @ weights - t_train)\n",
      "/tmp/ipykernel_45479/736322123.py:37: RuntimeWarning: invalid value encountered in matmul\n",
      "  change = eta / k *  X_train.T @ (X_train @ weights - t_train)\n",
      "/tmp/ipykernel_45479/736322123.py:46: RuntimeWarning: invalid value encountered in greater\n",
      "  return score>threshold\n"
     ]
    }
   ],
   "source": [
    "def add_bias(X):\n",
    "    \"\"\"X is a Nxm matrix: N datapoints, m features\n",
    "    Return a Nx(m+1) matrix with added bias in position zero\"\"\"\n",
    "    sh = X.shape\n",
    "    m = sh[0]\n",
    "    bias = np.ones((m,1)) # Make a m*1 matrix of 1-s\n",
    "    # Concatenate the column of bias in front of the columns of X.\n",
    "    return np.concatenate((bias, X), axis = 1)\n",
    "\n",
    "def add_non_linear_features(X):\n",
    "    \"\"\"X is a Nxm matrix: N datapoints, m features\n",
    "    Return a Nx(m+3) matrix with three added linear features in position m+1,m+2,m+3\n",
    "    these will be x_1^2, x_2^2 and x_1*x_2 all consecutive x_i's will be unused, \n",
    "    it is assumed that X already has x_0 as a bias weight\"\"\"\n",
    "    sh = X.shape\n",
    "    m = sh[0]\n",
    "    # We are adding 3 linear features to all m rows\n",
    "    non_linear_features = np.zeros((m,3))\n",
    "    for i in range(m):\n",
    "        non_linear_features[i][0] = X[i][1]**2\n",
    "        non_linear_features[i][1] = X[i][2]**2\n",
    "        non_linear_features[i][2] = X[i][1]*X[i][2]\n",
    "    return np.concatenate((X, non_linear_features), axis = 1)\n",
    "    \n",
    "\n",
    "class NumpyLinRegClass(NumpyClassifier):\n",
    "\n",
    "    def fit(self, X_train, t_train, eta = 0.1, epochs=10, diff=0.001):\n",
    "        \"\"\"X_train is a Nxm matrix, N data points, m features\n",
    "        t_train are the targets values for training data\"\"\"\n",
    "        \n",
    "        X_train = add_non_linear_features(add_bias(X_train))\n",
    "        (m, n) = X_train.shape\n",
    "        self.weights = weights = np.zeros(n)\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            change = eta / k *  X_train.T @ (X_train @ weights - t_train)     \n",
    "            weights -= change\n",
    "            \n",
    "            if sum(abs(weights)) < diff:\n",
    "                break\n",
    "    \n",
    "    def predict(self, x, threshold=0.5):\n",
    "        z = add_non_linear_features(add_bias(x))\n",
    "        score = z @ self.weights\n",
    "        return score>threshold    \n",
    "\n",
    "thresholds = [1e-9, 0.1, 0.2, 0.3]\n",
    "add_non_linear_features(add_bias(X_train))\n",
    "for x in thresholds:\n",
    "    cls = NumpyLinRegClass()\n",
    "    cls.fit(X_train, t2_train, epochs=1000, diff=x)\n",
    "    print(cls.accuracy(X_val, t2_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with linear features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48\n",
      "0.48\n",
      "0.48\n",
      "0.48\n",
      "0.48\n"
     ]
    }
   ],
   "source": [
    "class NumpyMultiLogReg(NumpyClassifier):\n",
    "\n",
    "    def fit(self, X_train, t_train, eta = 0.1, epochs=10, diff=0.001):\n",
    "        \"\"\"X_train is a Nxm matrix, N data points, m features\n",
    "        t_train are the targets values for training data\n",
    "        \n",
    "        Makes a fit for each feature, t_train now has multiple values\"\"\"\n",
    "        \n",
    "        X_train = add_non_linear_features(add_bias(X_train))\n",
    "        (m, n) = X_train.shape\n",
    "        \n",
    "        self.weights = weights = np.zeros(n)\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            change = eta / k *  X_train.T @ (self.forward(X_train) - t_train)\n",
    "            weights -= change\n",
    "            \n",
    "            if sum(abs(change)) < diff:\n",
    "                break\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return logistic(X @ self.weights)\n",
    "    \n",
    "    def score(self, x):\n",
    "        z = add_non_linear_features(add_bias(x))\n",
    "        score = self.forward(z)\n",
    "        return score\n",
    "    \n",
    "    def predict(self, x, threshold=0.5):\n",
    "        z = add_non_linear_features(add_bias(x))\n",
    "        score = self.forward(z)\n",
    "        return (score>threshold).astype('int')\n",
    "    \n",
    "thresholds = [1e-09, 0.001, 0.004, 0.01, 0.1]\n",
    "\n",
    "for x in thresholds:\n",
    "    cls = NumpyMultiLogReg()\n",
    "    cls.fit(X_train, t_train, epochs=1000, diff=x)\n",
    "    print(cls.accuracy(X_val, t_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron with non linear features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61\n"
     ]
    }
   ],
   "source": [
    "class PyPerClassifier(PyClassifier):\n",
    "    \"\"\"Simple perceptron python classifier\"\"\"\n",
    "    \n",
    "    def fit(self, X_train, y_train, eta=1, epochs=1):\n",
    "        \"\"\"Train the self.weights on the training data with learning\n",
    "        rate eta, running epochs many epochs\"\"\"\n",
    "        # adding the non linear features\n",
    "        X_train = add_non_linear_features(add_bias(np.array(X_train)))\n",
    "        X_train = [list(x) for x in X_train] # Put bias in position 0\n",
    "    \n",
    "        self.dim = dim = len(X_train[0])\n",
    "        self.weights = weights = [0 for _ in range(dim)]\n",
    "        # Initialize all weights to 0.\n",
    "\n",
    "        for e in range(epochs):\n",
    "            for x, t in zip(X_train, y_train):\n",
    "                y = int(self.forward(x)>0)\n",
    "                for i in range(dim):\n",
    "                    weights[i] -= eta * (y - t) * x[i]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Calculate the score for the item x\"\"\"\n",
    "        score = sum([self.weights[i]*x[i] for i in range(self.dim)])\n",
    "        return score       \n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict the value for the item x\"\"\"\n",
    "        # adding the non linear features for each predicted x\n",
    "        x = [1] + list(x) + [x[0]**2] + [x[1]**2] + [x[0]*x[1]]\n",
    "        score = self.forward(x)\n",
    "        return int(score > 0)\n",
    "\n",
    "# testing\n",
    "cls = PyPerClassifier()\n",
    "cls.fit(list(X_train), list(t2_train))\n",
    "print(cls.accuracy(list(X_val), list(t2_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kNN already uses non linear features so we will skip this one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Classifier</th>\n",
    "    <th class=\"tg-0pky\">Linear regression</th>\n",
    "    <th class=\"tg-0pky\">Logistic regression</th>\n",
    "    <th class=\"tg-0pky\">Perceptron</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Without linear features</td>\n",
    "    <td class=\"tg-0pky\">0.6075</td>\n",
    "    <td class=\"tg-0pky\">0.59</td>\n",
    "    <td class=\"tg-0pky\">0.6475</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">With linear features</td>\n",
    "    <td class=\"tg-0lax\">0.5275</td>\n",
    "    <td class=\"tg-0lax\">0.48</td>\n",
    "    <td class=\"tg-0lax\">0.61</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general the classifiers perform worse with the non linear features, but the perceptron is the one that is the least affected with its accuracy only being reduced by 3.75 percentage points and the logistic regression is the most affected, being reduced by 11 percentage points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II\n",
    "## Multi-layer neural networks\n",
    "We will implement the Multi-layer feed forward network (MLP, Marsland sec. 4.2.1). We will do so in two steps. In the first step, we will work concretely with the dataset (X, t). We will initialize the network and run a first round of training, i.e. one pass throught the algorithm at p. 78 in Marsland.\n",
    "\n",
    "In the second step, we will turn this code into a more general classifier. We can train and test this on (X, t), but also on other datasets.\n",
    "\n",
    "First of all we should scale the X, here I use a min-max scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a min-max scaler\n",
    "\n",
    "x_max = np.max(X_train)\n",
    "x_min = np.min(X_train)\n",
    "X_scaled = (X_train - x_min)/(x_max - x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: One round of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "We will only use one hidden layer. The number of nodes in the hidden layer will be a hyper-parameter provided by the user; let's call it *dim_hidden*. Initially, we will set it to 6. This is a hyper-parameter where other values may give better results, and the hyper-parameter could be tuned.\n",
    "\n",
    "Another hyper-parameter set by the user is the learning rate. We set the initial value to 0.01, but also this may need tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.01 #Learning rate\n",
    "dim_hidden = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the input *X_train* (after scaling) is a matrix of dimension *P x dim_in*, where *P* is the number of training instances, and *dim_in* is the number of features in the training instances. Hence we can read *dim_in* off from *X_train*. Similarly, we can read *dim_out* off from *t_train*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# amount of features\n",
    "dim_in =  X_train.shape[1]  # Calculate the correct value from the input data\n",
    "\n",
    "dim_out = len(set(t_train))\n",
    "\n",
    "# convert t_train to one-hot-encoding\n",
    "hot_map = {0: [1,0,0], 1: [0,1,0], 2: [0,0,1]}\n",
    "t_hot = [hot_map[x] for x in t_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need two sets of weights: weights1 between the input and the hidden layer, and weights2, between the hidden layer and the output. We will create the weight matrices and initialize them to small random numbers. Augmenting the dimension by one to get the correct dimensions with the bias terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add +1 for the bias\n",
    "weights1 = np.zeros((dim_in+1, dim_hidden))\n",
    "weights2 = np.zeros((dim_hidden+1, dim_out))\n",
    "\n",
    "# setting the bias to 1\n",
    "weights1[0] = weights2[0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forwards phase\n",
    "We will run the first step in the training, and start with the forward phase. Calculate the activations after the hidden layer and after the output layer. We will use the logistic (sigmoid) activation function in both layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_activations = add_bias(X_scaled) @ weights1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp = add_bias(X_scaled)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def forward(X):\n",
    "    H_j = X @ weights1\n",
    "    A = sigmoid(H_j)\n",
    "    A_b = add_bias(A)\n",
    "    H_k = A_b @ weights2\n",
    "    \n",
    "    return sigmoid(H_k)\n",
    "\n",
    "output_activations = forward(X_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backwards phase\n",
    "Here we calculate the delta terms at the output. We assume that we use sums of squared errors. (This amounts to the same as using the mean square error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_k = forward(add_bias(X_scaled))\n",
    "delta_o = (y_k - t_hot)*y_k*(1-y_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate the delta terms in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def backward(X):\n",
    "    y_k = forward(X)\n",
    "    delta_o = (y_k - t_hot)*y_k*(1-y_k)\n",
    "    H_j = X @ weights1\n",
    "    A = sigmoid(H_j)\n",
    "    A = add_bias(A)\n",
    "    delta_h = A * (1-A) * (delta_o @ weights2.T)\n",
    "    return delta_h\n",
    "\n",
    "\n",
    "X_temp = forward(add_bias(X_scaled))\n",
    "backward(X_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update the weights and check that they have changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights 2 before update\n",
      " [[1. 1. 1.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]] \n",
      "\n",
      "\n",
      "after update\n",
      " [[ 0.23154843  0.66802692  0.22368395]\n",
      " [-0.56178311 -0.24269177 -0.56753251]\n",
      " [-0.56178311 -0.24269177 -0.56753251]\n",
      " [-0.56178311 -0.24269177 -0.56753251]\n",
      " [-0.56178311 -0.24269177 -0.56753251]\n",
      " [-0.56178311 -0.24269177 -0.56753251]\n",
      " [-0.56178311 -0.24269177 -0.56753251]] \n",
      "\n",
      "weights 1 before update\n",
      " [[1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]] \n",
      "\n",
      "\n",
      "after update\n",
      " [[1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#update weights\n",
    "X_temp = add_bias(X_scaled)\n",
    "y_k = forward(X_temp)\n",
    "delta_o = (y_k - t_hot)*y_k*(1-y_k)\n",
    "\n",
    "H_j = X_temp @ weights1\n",
    "A = sigmoid(H_j)\n",
    "A_b = add_bias(A)\n",
    "H_k = A_b @ weights2\n",
    "delta_h = A_b * (1-A_b) * (delta_o @ weights2.T)\n",
    "\n",
    "print(\"weights 2 before update\\n\", weights2, \"\\n\\n\")\n",
    "weights2 = weights2 - eta*A_b.T@delta_o \n",
    "print(\"after update\\n\",weights2, \"\\n\")\n",
    "\n",
    "print(\"weights 1 before update\\n\", weights1, \"\\n\\n\")\n",
    "# not including the bias by [:,:-1]\n",
    "weights1 = weights1 - eta*X_temp.T@delta_h[:,:-1]\n",
    "print(\"after update\\n\",weights1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 2: A Multi-layer neural network classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train and test a classifier on (X, t). To be true to the DRY principle we will build a general Multi-layer neural network classfier as a class. This class will have some of the same structure as the classifiers we made for linear and logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring that I will get an error if i accidentally use any of the variables declared outside of the MNN class\n",
    "A = y_k = delta_o = delta_h = eta = weights1 = weights2 = H_j = H_k = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45479/1586608329.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  233\n",
      "0.5825\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def add_bias(X):\n",
    "    \"\"\"X is a Nxm matrix: N datapoints, m features\n",
    "    Return a Nx(m+1) matrix with added bias in position zero\"\"\"\n",
    "    sh = X.shape\n",
    "    m = sh[0]\n",
    "    bias = np.ones((m,1)) # Make a m*1 matrix of 1-s\n",
    "    # Concatenate the column of bias in front of the columns of X.\n",
    "    return np.concatenate((bias, X), axis = 1)\n",
    "\n",
    "def one_hot_encoding(t):\n",
    "    \"\"\"Converts integer values to one hot encoding\"\"\"\n",
    "    hot_map = {0: [1,0,0], 1: [0,1,0], 2: [0,0,1]}\n",
    "    return np.array([hot_map[x] for x in t])\n",
    "\n",
    "def scale_min_max(X):\n",
    "    \"\"\"min-max scaler\"\"\"\n",
    "    x_max = np.max(X)\n",
    "    x_min = np.min(X)\n",
    "    X_scaled = (X - x_min)/(x_max - x_min)\n",
    "    return X_scaled\n",
    "    \n",
    "class MNNClassifier():\n",
    "    \"\"\"A multi-layer neural network with one hidden layer\"\"\"\n",
    "    \n",
    "    def __init__(self, eta = 0.001, dim_hidden = 6):\n",
    "        \"\"\"Initialize the hyperparameters\"\"\"\n",
    "        self.eta = eta\n",
    "        self.dim_hidden = dim_hidden\n",
    "        \n",
    "    def fit(self, X_train, t_train, epochs = 100):\n",
    "        \"\"\"Initialize the self.weights. Train *epochs* many epochs.\"\"\"\n",
    "        # Initilaization\n",
    "        \n",
    "        #scaling X\n",
    "        X_train = scale_min_max(X_train)\n",
    "        \n",
    "        # convert t_train to one-hot-encoding\n",
    "        self.t_hot = one_hot_encoding(t_train)\n",
    "        \n",
    "        # amount of features\n",
    "        dim_in =  X_train.shape[1]  # Calculate the correct value from the input data\n",
    "        self.dim_out = self.t_hot.shape[1]\n",
    "        \n",
    "        # add +1 for the bias\n",
    "        self.weights1 = np.ones((dim_in+1, self.dim_hidden))/1e3\n",
    "        self.weights2 = np.ones((self.dim_hidden+1, self.dim_out))/1e3\n",
    "\n",
    "        self.X_b = X_b = add_bias(X_train)\n",
    "        self.forward(self.X_b)\n",
    "        self.backward()\n",
    "        self.update()\n",
    "\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            # Run one epoch of forward-backward\n",
    "            self.forward(self.X_b)\n",
    "            self.backward()\n",
    "            self.update()\n",
    "\n",
    "            \n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform one forward step. \n",
    "        Return a pair consisting of the outputs of the hidden_layer\n",
    "        and the outputs on the final layer\"\"\"\n",
    "        self.H_j = X @ self.weights1\n",
    "        A = sigmoid(self.H_j)\n",
    "        self.A_b = add_bias(A)\n",
    "        H_k = self.A_b @ self.weights2\n",
    "        self.y_k = sigmoid(H_k)\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Perform one backward step.\"\"\"\n",
    "        self.delta_o = (self.y_k - self.t_hot)*self.y_k*(1-self.y_k)\n",
    "        self.delta_h = self.A_b * (1-self.A_b) * (self.delta_o @ self.weights2.T)\n",
    "        \n",
    "    def update(self):\n",
    "        \"\"\"Update both sets of weights for the network\"\"\"\n",
    "        self.weights2 = self.weights2 - self.eta*self.A_b.T@self.delta_o \n",
    "        self.weights1 = self.weights1 - self.eta*self.X_b.T@self.delta_h[:,:-1]\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"predict the feature for every point in X_test\n",
    "        Return predicted\"\"\"\n",
    "        X_tb = add_bias(X_test)\n",
    "        self.forward(X_tb)\n",
    "        predicted = np.zeros((len(X_test),self.dim_out))\n",
    "        for i in range(0,len(X_test)):\n",
    "            # make the guess the one with the highest probability from y_k\n",
    "            predicted[i][np.argmax(self.y_k[i])] = 1\n",
    "        return predicted\n",
    "    \n",
    "    def accuracy(self, X_test, t_test):\n",
    "        \"\"\"Calculate the accuracy of the classifier for the pair (X_test, t_test)\n",
    "        Return the accuracy\"\"\"\n",
    "        \n",
    "        t_test = one_hot_encoding(t_test)\n",
    "        pred = self.predict(X_test)\n",
    "        correct = 0\n",
    "        # calculate amount correctly predicted\n",
    "        for i in range(len(t_test)):\n",
    "            if list(t_test[i]) == list(pred[i]):\n",
    "                correct += 1\n",
    "        print(\"correct: \", correct)\n",
    "        return correct/len(pred)\n",
    "\n",
    "cls = MNNClassifier(eta = 0.1)\n",
    "cls.fit(X_train, t_train, epochs = 10000)\n",
    "print(cls.accuracy(X_val, t_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train the network on (X_train, t_train) (after scaling), and test on (X_val, t_val)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A neural network classifier for (X, t2)\n",
    "Let us see whether a multilayer neural network can learn a non-linear classifier.<br>\n",
    "We will train it on (X_train, t2_train), test it on (X_val, t2_val) and tune the hyper-parameters for the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45479/1586608329.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  254\n",
      "0.635\n"
     ]
    }
   ],
   "source": [
    "cls = MNNClassifier(eta = 0.1)\n",
    "cls.fit(X_train, t2_train, epochs=10000)\n",
    "print(cls.accuracy(X_val, t2_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Final testing\n",
    "Here we take the best classifiers that we've found for the training sets (X, t) and (X, t2) and test them on (X_test, t_test) and (X_test, t2_test), respectively. Compute accuracy, the confusion matrix, precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "kNN : 0.7575 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45479/1586608329.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  240\n",
      "MNN : 0.6\n"
     ]
    }
   ],
   "source": [
    "cls = PykNNClassifier(20)\n",
    "cls.fit(X_train, t_train)\n",
    "print(\"\\nkNN :\",cls.accuracy(X_test, t_test),\"\\n\")\n",
    "\n",
    "cls = MNNClassifier(eta=0.1)\n",
    "cls.fit(X_train, t_train, epochs=10000)\n",
    "print(\"MNN :\",cls.accuracy(X_test, t_test))\n",
    "\n",
    "# For the kNN classifier, the accuracy is only a percentage point worse than for the validation sets, X_val and t_val. \n",
    "# The MNN actually did a tiny 0.05 bit better for the test set than for (X_val, t_val).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45479/1586608329.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            truth\n",
      "                    |  negative|  positive|\n",
      "          ---------------------------------\n",
      "          negative  |      50.0|       5.0|\n",
      "predicted ---------------------------------\n",
      "          positive  |     155.0|     190.0|\n",
      "          ---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "def cf_matrix(predicted, truth):\n",
    "    table = np.zeros((2,2))\n",
    "    for p,g in zip(predicted, truth):\n",
    "        if list(p) == list(g) == [0,1,0]:\n",
    "            table[1, 1] +=1\n",
    "        elif list(p) == [0,1,0]:\n",
    "            table[1, 0] += 1\n",
    "        elif list(g) == [0,1,0]:\n",
    "            table[0, 1] += 1\n",
    "        else:\n",
    "            table[0,0] += 1\n",
    "    print(28*\" \"+\"truth\")\n",
    "    print(\"{:20}|{:>10}|{:>10}|\".format(\" \",\"negative\", \"positive\"))\n",
    "    print(10*\" \"+33*\"-\")\n",
    "    print(\"{:10}{:10}|{:10}|{:10}|\".format(\" \",\"negative\",table[0,0], table[0,1]))\n",
    "    print(\"predicted \"+33*\"-\")\n",
    "    print(\"{:10}{:10}|{:10}|{:10}|\".format(\" \",\"positive\",table[1,0], table[1,1]))\n",
    "    print(10*\" \"+33*\"-\")\n",
    "    \n",
    "cls = MNNClassifier(eta=0.1)\n",
    "cls.fit(X_train, t_train, epochs = 10000)\n",
    "predicted = cls.predict(X_test)\n",
    "cf_matrix(predicted, one_hot_encoding(t_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
